<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Marzieh Fadaee</title> <meta name="author" content="Marzieh Fadaee"> <meta name="description" content="&lt;h6&gt;An up-to-date list is available on my &lt;b&gt;&lt;a href='https://scholar.google.com/citations?hl=en&amp;user=NZqs0toAAAAJ&amp;view_op=list_works&amp;sortby=pubdate'&gt;Google Scholar&lt;/a&gt;&lt;/b&gt;.&lt;/h6&gt;&lt;hr&gt;"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://marziehf.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?f9d4e7248e54e0dde2dddcb251b631cc"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Marzieh </span>Fadaee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> <h6>An up-to-date list is available on my <b><a href="https://scholar.google.com/citations?hl=en&amp;user=NZqs0toAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Google Scholar</a></b>.</h6> <hr> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="peppin2025multilingualdivideimpactglobal" class="col-sm-8"> <div class="title">The Multilingual Divide and Its Impact on Global AI Safety</div> <div class="author"> Aidan Peppin, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, Alice Schoenauer Sebag, <a href="https://kellymarchisio.github.io/" rel="external nofollow noopener" target="_blank">Kelly Marchisio</a>, Beyza Ermis, John Dang, Samuel Cahyawijaya, Shivalika Singh, Seraphina Goldfarb-Tarrant, Viraat Aryabumi,  Aakanksha, Wei-Yin Ko, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, Matthias Gallé, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.21344" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">peppin2025multilingualdivideimpactglobal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Multilingual Divide and Its Impact on Global AI Safety}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Peppin, Aidan and Kreutzer, Julia and Sebag, Alice Schoenauer and Marchisio, Kelly and Ermis, Beyza and Dang, John and Cahyawijaya, Samuel and Singh, Shivalika and Goldfarb-Tarrant, Seraphina and Aryabumi, Viraat and Aakanksha and Ko, Wei-Yin and Üstün, Ahmet and Gallé, Matthias and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2505.21344}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="schwartz2025realitychecknewevaluation" class="col-sm-8"> <div class="title">Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI’s Real World Effects</div> <div class="author"> Reva Schwartz, Rumman Chowdhury, Akash Kundu, Heather Frase, <em>Marzieh Fadaee</em>, Tom David, Gabriella Waters, Afaf Taik, Morgan Briggs, Patrick Hall, Shomik Jain, Kyra Yee, Spencer Thomas, Sundeep Bhandari, Qinghua Lu, Matthew Holmes, and Theodora Skeadas</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.18893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">schwartz2025realitychecknewevaluation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schwartz, Reva and Chowdhury, Rumman and Kundu, Akash and Frase, Heather and Fadaee, Marzieh and David, Tom and Waters, Gabriella and Taik, Afaf and Briggs, Morgan and Hall, Patrick and Jain, Shomik and Yee, Kyra and Thomas, Spencer and Bhandari, Sundeep and Lu, Qinghua and Holmes, Matthew and Skeadas, Theodora}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2505.18893}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CY}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="dash2025ayavisionadvancingfrontier" class="col-sm-8"> <div class="title">Aya Vision: Advancing the Frontier of Multilingual Multimodality</div> <div class="author"> Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, <a href="https://scholar.google.com/citations?user=eJwbbXEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Phil Blunsom</a>, Aidan Gomez, Ivan Zhang, <em>Marzieh Fadaee</em>, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.08751" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/collections/CohereLabs/cohere-labs-aya-vision-67c4ccd395ca064308ee1484" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://huggingface.co/CohereLabs/aya-vision-32b" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">dash2025ayavisionadvancingfrontier</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aya Vision: Advancing the Frontier of Multilingual Multimodality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dash, Saurabh and Nan, Yiyang and Dang, John and Ahmadian, Arash and Singh, Shivalika and Smith, Madeline and Venkitesh, Bharat and Shmyhlo, Vlad and Aryabumi, Viraat and Beller-Morales, Walter and Pekmez, Jeremy and Ozuzu, Jason and Richemond, Pierre and Locatelli, Acyr and Frosst, Nick and Blunsom, Phil and Gomez, Aidan and Zhang, Ivan and Fadaee, Marzieh and Govindassamy, Manoj and Roy, Sudip and Gallé, Matthias and Ermis, Beyza and Üstün, Ahmet and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2505.08751}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="singh2025leaderboardillusion" class="col-sm-8"> <div class="title">The Leaderboard Illusion</div> <div class="author"> Shivalika Singh, Yiyang Nan, <a href="https://w4ngatang.github.io/" rel="external nofollow noopener" target="_blank">Alex Wang</a>, Daniel D’Souza, Sayash Kapoor, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, Sanmi Koyejo, Yuntian Deng, <a href="https://www.shaynelongpre.com/" rel="external nofollow noopener" target="_blank">Shayne Longpre</a>, <a href="https://nasmith.github.io/" rel="external nofollow noopener" target="_blank">Noah A. Smith</a>, Beyza Ermis, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.20879" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://cohere.com/research/lmarena" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2025leaderboardillusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Leaderboard Illusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Shivalika and Nan, Yiyang and Wang, Alex and D'Souza, Daniel and Kapoor, Sayash and Üstün, Ahmet and Koyejo, Sanmi and Deng, Yuntian and Longpre, Shayne and Smith, Noah A. and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.20879}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="shimabucoro2025posttrainersguidemultilingualtraining" class="col-sm-8"> <div class="title">A Post-trainer’s Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics</div> <div class="author"> Luisa Shimabucoro, Ahmet Ustun, <em>Marzieh Fadaee</em>, and <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.16677" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">shimabucoro2025posttrainersguidemultilingualtraining</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shimabucoro, Luisa and Ustun, Ahmet and Fadaee, Marzieh and Ruder, Sebastian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.16677}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="kreutzer2025dejavumultilingualllm" class="col-sm-8"> <div class="title">Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</div> <div class="author"> <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, Eleftheria Briakou, Sweta Agrawal, <em>Marzieh Fadaee</em>, and Kocmi Tom</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.11829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">kreutzer2025dejavumultilingualllm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kreutzer, Julia and Briakou, Eleftheria and Agrawal, Sweta and Fadaee, Marzieh and Tom, Kocmi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.11829}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="salazar2025kaleidoscopeinlanguageexamsmassively" class="col-sm-8"> <div class="title">Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</div> <div class="author"> Israfel Salazar, Manuel Fernández Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemiński, Jekaterina Novikova, Luísa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popovič, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel Costa Merlin, Otávio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin fard, Silvia Fernandez, María Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.07072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/kaleidoscope" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">salazar2025kaleidoscopeinlanguageexamsmassively</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Salazar, Israfel and Burda, Manuel Fernández and Islam, Shayekh Bin and Moakhar, Arshia Soltani and Singh, Shivalika and Farestam, Fabian and Romanou, Angelika and Boiko, Danylo and Khullar, Dipika and Zhang, Mike and Krzemiński, Dominik and Novikova, Jekaterina and Shimabucoro, Luísa and Imperial, Joseph Marvin and Maheshwary, Rishabh and Duwal, Sharad and Amayuelas, Alfonso and Rajwal, Swati and Purbey, Jebish and Ruby, Ahmed and Popovič, Nicholas and Suppa, Marek and Wasi, Azmine Toushik and Kadiyala, Ram Mohan Rao and Tsymboi, Olga and Kostritsya, Maksim and Moakhar, Bardia Soltani and da Costa Merlin, Gabriel and Coletti, Otávio Ferracioli and Shiviari, Maral Jabbari and farahani fard, MohammadAmin and Fernandez, Silvia and Grandury, María and Abulkhanov, Dmitry and Sharma, Drishti and Mitri, Andre Guarnier De and Marchezi, Leticia Bossatto and Heydari, Setayesh and Obando-Ceron, Johan and Kohut, Nazar and Ermis, Beyza and Elliott, Desmond and Ferrante, Enzo and Hooker, Sara and Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.07072}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="cohere2025commandaenterprisereadylarge" class="col-sm-8"> <div class="title">Command A: An Enterprise-Ready Large Language Model</div> <div class="author"> Team Cohere,  Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, Neeral Beladia, Walter Beller-Morales, Alexandre Bérard, Andrew Berneshawi, Anna Bialas, <a href="https://scholar.google.com/citations?user=eJwbbXEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Phil Blunsom</a>, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagné, Julián Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D’souza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Théo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Ergün, Beyza Ermis, <em>Marzieh Fadaee</em>, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gallé, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Ellen Gilsenan-McMahon, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofstätter, Sungjin Hong, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, <a href="https://eddotman.github.io/" rel="external nofollow noopener" target="_blank">Edward Kim</a>, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kryściński, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Lukáš Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, <a href="https://kellymarchisio.github.io/" rel="external nofollow noopener" target="_blank">Kelly Marchisio</a>, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, Cécile Robert-Michon, Aurélien Rodriguez, Sudip Roy, <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a>, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Shang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, <a href="https://w4ngatang.github.io/" rel="external nofollow noopener" target="_blank">Alex Wang</a>, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, and Zhoujie Zhao</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.00698" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/CohereLabs/c4ai-command-a-03-2025" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">cohere2025commandaenterprisereadylarge</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Command A: An Enterprise-Ready Large Language Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cohere, Team and Aakanksha and Ahmadian, Arash and Ahmed, Marwan and Alammar, Jay and Alizadeh, Milad and Alnumay, Yazeed and Althammer, Sophia and Arkhangorodsky, Arkady and Aryabumi, Viraat and Aumiller, Dennis and Avalos, Raphaël and Aviv, Zahara and Bae, Sammie and Baji, Saurabh and Barbet, Alexandre and Bartolo, Max and Bebensee, Björn and Beladia, Neeral and Beller-Morales, Walter and Bérard, Alexandre and Berneshawi, Andrew and Bialas, Anna and Blunsom, Phil and Bobkin, Matt and Bongale, Adi and Braun, Sam and Brunet, Maxime and Cahyawijaya, Samuel and Cairuz, David and Campos, Jon Ander and Cao, Cassie and Cao, Kris and Castagné, Roman and Cendrero, Julián and Currie, Leila Chan and Chandak, Yash and Chang, Diane and Chatziveroglou, Giannis and Chen, Hongyu and Cheng, Claire and Chevalier, Alexis and Chiu, Justin T. and Cho, Eugene and Choi, Eugene and Choi, Eujeong and Chung, Tim and Cirik, Volkan and Cismaru, Ana and Clavier, Pierre and Conklin, Henry and Crawhall-Stein, Lucas and Crouse, Devon and Cruz-Salinas, Andres Felipe and Cyrus, Ben and D'souza, Daniel and Dalla-Torre, Hugo and Dang, John and Darling, William and Domingues, Omar Darwiche and Dash, Saurabh and Debugne, Antoine and Dehaze, Théo and Desai, Shaan and Devassy, Joan and Dholakia, Rishit and Duffy, Kyle and Edalati, Ali and Eldeib, Ace and Elkady, Abdullah and Elsharkawy, Sarah and Ergün, Irem and Ermis, Beyza and Fadaee, Marzieh and Fan, Boyu and Fayoux, Lucas and Flet-Berliac, Yannis and Frosst, Nick and Gallé, Matthias and Galuba, Wojciech and Garg, Utsav and Geist, Matthieu and Azar, Mohammad Gheshlaghi and Gilsenan-McMahon, Ellen and Goldfarb-Tarrant, Seraphina and Goldsack, Tomas and Gomez, Aidan and Gonzaga, Victor Machado and Govindarajan, Nithya and Govindassamy, Manoj and Grinsztajn, Nathan and Gritsch, Nikolas and Gu, Patrick and Guo, Shangmin and Haefeli, Kilian and Hajjar, Rod and Hawes, Tim and He, Jingyi and Hofstätter, Sebastian and Hong, Sungjin and Hooker, Sara and Hosking, Tom and Howe, Stephanie and Hu, Eric and Huang, Renjie and Jain, Hemant and Jain, Ritika and Jakobi, Nick and Jenkins, Madeline and Jordan, JJ and Joshi, Dhruti and Jung, Jason and Kalyanpur, Trushant and Kamalakara, Siddhartha Rao and Kedrzycki, Julia and Keskin, Gokce and Kim, Edward and Kim, Joon and Ko, Wei-Yin and Kocmi, Tom and Kozakov, Michael and Kryściński, Wojciech and Jain, Arnav Kumar and Teru, Komal Kumar and Land, Sander and Lasby, Michael and Lasche, Olivia and Lee, Justin and Lewis, Patrick and Li, Jeffrey and Li, Jonathan and Lin, Hangyu and Locatelli, Acyr and Luong, Kevin and Ma, Raymond and Mach, Lukáš and Machado, Marina and Magbitang, Joanne and Lopez, Brenda Malacara and Mann, Aryan and Marchisio, Kelly and Markham, Olivia and Matton, Alexandre and McKinney, Alex and McLoughlin, Dominic and Mokry, Jozef and Morisot, Adrien and Moulder, Autumn and Moynehan, Harry and Mozes, Maximilian and Muppalla, Vivek and Murakhovska, Lidiya and Nagarajan, Hemangani and Nandula, Alekhya and Nasir, Hisham and Nehra, Shauna and Netto-Rosen, Josh and Ohashi, Daniel and Owers-Bardsley, James and Ozuzu, Jason and Padilla, Dennis and Park, Gloria and Passaglia, Sam and Pekmez, Jeremy and Penstone, Laura and Piktus, Aleksandra and Ploeg, Case and Poulton, Andrew and Qi, Youran and Raghvendra, Shubha and Ramos, Miguel and Ranjan, Ekagra and Richemond, Pierre and Robert-Michon, Cécile and Rodriguez, Aurélien and Roy, Sudip and Ruder, Sebastian and Ruis, Laura and Rust, Louise and Sachan, Anubhav and Salamanca, Alejandro and Saravanakumar, Kailash Karthik and Satyakam, Isha and Sebag, Alice Schoenauer and Sen, Priyanka and Sepehri, Sholeh and Seshadri, Preethi and Shen, Ye and Sherborne, Tom and Shi, Sylvie Shang and Shivaprasad, Sanal and Shmyhlo, Vladyslav and Shrinivason, Anirudh and Shteinbuk, Inna and Shukayev, Amir and Simard, Mathieu and Snyder, Ella and Spataru, Ava and Spooner, Victoria and Starostina, Trisha and Strub, Florian and Su, Yixuan and Sun, Jimin and Talupuru, Dwarak and Tarassov, Eugene and Tommasone, Elena and Tracey, Jennifer and Trend, Billy and Tumer, Evren and Üstün, Ahmet and Venkitesh, Bharat and Venuto, David and Verga, Pat and Voisin, Maxime and Wang, Alex and Wang, Donglu and Wang, Shijian and Wen, Edmond and White, Naomi and Willman, Jesse and Winkels, Marysia and Xia, Chen and Xie, Jessica and Xu, Minjie and Yang, Bowen and Yi-Chern, Tan and Zhang, Ivan and Zhao, Zhenyu and Zhao, Zhoujie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.00698}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="rakotonirina2025toolsteammatesevaluatingllms" class="col-sm-8"> <div class="title">From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions</div> <div class="author"> Nathanaël Carraz Rakotonirina, Mohammed Hamdy, Jon Ander Campos, Lucas Weber, Alberto Testoni, <em>Marzieh Fadaee</em>, Sandro Pezzelle, and Marco Del Tredici</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2502.13791" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabsCommunity/memorycode" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://github.com/Cohere-Labs-Community/MemoryCode" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">rakotonirina2025toolsteammatesevaluatingllms</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rakotonirina, Nathanaël Carraz and Hamdy, Mohammed and Campos, Jon Ander and Weber, Lucas and Testoni, Alberto and Fadaee, Marzieh and Pezzelle, Sandro and Tredici, Marco Del}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2502.13791}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="baack2025bestpracticesopendatasets" class="col-sm-8"> <div class="title">Towards Best Practices for Open Datasets for LLM Training</div> <div class="author"> Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, <em>Marzieh Fadaee</em>, Lisa Gutermuth, Hynek Kydlíček, Greg Leppert, EM Lewis-Jong, Solana Larsen, <a href="https://www.shaynelongpre.com/" rel="external nofollow noopener" target="_blank">Shayne Longpre</a>, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu, Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White, and Thomas Wolf</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2501.08365" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {Towards Best Practices for Open Datasets for LLM Training},</span>
  <span class="c">author = {Baack, Stefan and Biderman, Stella and Odrozek, Kasia and Skowron, Aviya and Bdeir, Ayah and Bommarito, Jillian and Ding, Jennifer and Gahntz, Maximilian and Keller, Paul and Langlais, Pierre-Carl and Lindahl, Greg and Majstorovic, Sebastian and Marda, Nik and Penedo, Guilherme and Segbroeck, Maarten Van and Wang, Jennifer and von Werra, Leandro and Baker, Mitchell and Belião, Julie and Chmielinski, Kasia and Fadaee, Marzieh and Gutermuth, Lisa and Kydlíček, Hynek and Leppert, Greg and Lewis-Jong, EM and Larsen, Solana and Longpre, Shayne and Lungati, Angela Oduor and Miller, Cullen and Miller, Victor and Ryabinin, Max and Siminyu, Kathleen and Strait, Andrew and Surman, Mark and Tumadóttir, Anna and Weber, Maurice and Weiss, Rebecca and White, Lee and Wolf, Thomas},</span>
  <span class="c">year = {2025},</span>
  <span class="c">eprint = {2501.08365},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CY},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="dang2024ayaexpansecombiningresearch" class="col-sm-8"> <div class="title">Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier</div> <div class="author"> John Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, <a href="https://eddotman.github.io/" rel="external nofollow noopener" target="_blank">Eddie Kim</a>, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, <a href="https://scholar.google.com/citations?user=eJwbbXEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Phil Blunsom</a>, Ivan Zhang, Aidan Gomez, Nick Frosst, <em>Marzieh Fadaee</em>, Beyza Ermis, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2412.04261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/collections/CohereLabs/cohere-labs-aya-expanse-671a83d6b2c07c692beab3c3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://huggingface.co/CohereLabs/aya-expanse-32b" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">dang2024ayaexpansecombiningresearch</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dang, John and Singh, Shivalika and D'souza, Daniel and Ahmadian, Arash and Salamanca, Alejandro and Smith, Madeline and Peppin, Aidan and Hong, Sungjin and Govindassamy, Manoj and Zhao, Terrence and Kublik, Sandra and Amer, Meor and Aryabumi, Viraat and Campos, Jon Ander and Tan, Yi-Chern and Kocmi, Tom and Strub, Florian and Grinsztajn, Nathan and Flet-Berliac, Yannis and Locatelli, Acyr and Lin, Hangyu and Talupuru, Dwarak and Venkitesh, Bharat and Cairuz, David and Yang, Bowen and Chung, Tim and Ko, Wei-Yin and Shi, Sylvie Shang and Shukayev, Amir and Bae, Sammie and Piktus, Aleksandra and Castagné, Roman and Cruz-Salinas, Felipe and Kim, Eddie and Crawhall-Stein, Lucas and Morisot, Adrien and Roy, Sudip and Blunsom, Phil and Zhang, Ivan and Gomez, Aidan and Frosst, Nick and Fadaee, Marzieh and Ermis, Beyza and Üstün, Ahmet and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2412.04261}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="singh2024globalmmluunderstandingaddressing" class="col-sm-8"> <div class="title">Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation</div> <div class="author"> Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, <a href="https://kellymarchisio.github.io/" rel="external nofollow noopener" target="_blank">Kelly Marchisio</a>, Wei Qi Leong, Yosephine Susanto, Raymond Ng, <a href="https://www.shaynelongpre.com/" rel="external nofollow noopener" target="_blank">Shayne Longpre</a>, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, <em>Marzieh Fadaee</em>, Beyza Ermis, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2412.03304" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/Global-MMLU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2024globalmmluunderstandingaddressing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Shivalika and Romanou, Angelika and Fourrier, Clémentine and Adelani, David I. and Ngui, Jian Gang and Vila-Suero, Daniel and Limkonchotiwat, Peerat and Marchisio, Kelly and Leong, Wei Qi and Susanto, Yosephine and Ng, Raymond and Longpre, Shayne and Ko, Wei-Yin and Smith, Madeline and Bosselut, Antoine and Oh, Alice and Martins, Andre F. T. and Choshen, Leshem and Ippolito, Daphne and Ferrante, Enzo and Fadaee, Marzieh and Ermis, Beyza and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2412.03304}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="romanou2024includeevaluatingmultilinguallanguage" class="col-sm-8"> <div class="title">INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</div> <div class="author"> Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, <em>Marzieh Fadaee</em>, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and Antoine Bosselut</div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2411.19799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/include-base-44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="/assets/pdf/romanou_iclr_2025_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">romanou2024includeevaluatingmultilinguallanguage</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Romanou, Angelika and Foroutan, Negar and Sotnikova, Anna and Chen, Zeming and Nelaturu, Sree Harsha and Singh, Shivalika and Maheshwary, Rishabh and Altomare, Micol and Haggag, Mohamed A. and A, Snegha and Amayuelas, Alfonso and Amirudin, Azril Hafizi and Aryabumi, Viraat and Boiko, Danylo and Chang, Michael and Chim, Jenny and Cohen, Gal and Dalmia, Aditya Kumar and Diress, Abraham and Duwal, Sharad and Dzenhaliou, Daniil and Florez, Daniel Fernando Erazo and Farestam, Fabian and Imperial, Joseph Marvin and Islam, Shayekh Bin and Isotalo, Perttu and Jabbarishiviari, Maral and Karlsson, Börje F. and Khalilov, Eldar and Klamm, Christopher and Koto, Fajri and Krzemiński, Dominik and de Melo, Gabriel Adriano and Montariol, Syrielle and Nan, Yiyang and Niklaus, Joel and Novikova, Jekaterina and Ceron, Johan Samir Obando and Paul, Debjit and Ploeger, Esther and Purbey, Jebish and Rajwal, Swati and Ravi, Selvan Sunitha and Rydell, Sara and Santhosh, Roshan and Sharma, Drishti and Skenduli, Marjana Prifti and Moakhar, Arshia Soltani and Moakhar, Bardia Soltani and Tamir, Ran and Tarun, Ayush Kumar and Wasi, Azmine Toushik and Weerasinghe, Thenuka Ovin and Yilmaz, Serhan and Zhang, Mike and Schlag, Imanol and Fadaee, Marzieh and Hooker, Sara and Bosselut, Antoine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2411.19799}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="gureja2024mrewardbenchevaluatingrewardmodels" class="col-sm-8"> <div class="title">M-RewardBench: Evaluating Reward Models in Multilingual Settings</div> <div class="author"> Srishti Gureja, Lester James V. Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a>, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.15522" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabsCommunity/multilingual-reward-bench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://github.com/Cohere-Labs-Community/m-rewardbench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {M-RewardBench: Evaluating Reward Models in Multilingual Settings},</span>
  <span class="c">author = {Gureja, Srishti and Miranda, Lester James V. and Islam, Shayekh Bin and Maheshwary, Rishabh and Sharma, Drishti and Winata, Gusti and Lambert, Nathan and Ruder, Sebastian and Hooker, Sara and Fadaee, Marzieh},</span>
  <span class="c">year = {2024},</span>
  <span class="c">eprint = {2410.15522},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CL},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Neurips Wokshop</abbr></div> <div id="aakanksha2024mixdatamergemodels" class="col-sm-8"> <div class="title">Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning</div> <div class="author"> Aakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.10801" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning},</span>
  <span class="c">author = {Aakanksha and Ahmadian, Arash and Goldfarb-Tarrant, Seraphina and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara},</span>
  <span class="c">year = {2024},</span>
  <span class="c">eprint = {2410.10801},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CL},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="yu2024diversifyconquerdiversitycentricdata" class="col-sm-8"> <div class="title">Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement</div> <div class="author"> Simon Yu, Liangyu Chen, Sara Ahmadian, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2409.11378" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/Cohere-Labs-Community/iterative-data-selection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement},</span>
  <span class="c">author = {Yu, Simon and Chen, Liangyu and Ahmadian, Sara and Fadaee, Marzieh},</span>
  <span class="c">year = {2024},</span>
  <span class="c">eprint = {2409.11378},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CL},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="aryabumi2024codecodeexploringimpact" class="col-sm-8"> <div class="title">To Code, or Not To Code? Exploring Impact of Code in Pre-training</div> <div class="author"> Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, <em>Marzieh Fadaee</em>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2408.10914" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/ICLR%202025%20-%20Poster%20Landscape%20-%20Code%20in%20pretraining.png" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {To Code, or Not To Code? Exploring Impact of Code in Pre-training},</span>
  <span class="c">author = {Aryabumi, Viraat and Su, Yixuan and Ma, Raymond and Morisot, Adrien and Zhang, Ivan and Locatelli, Acyr and Fadaee, Marzieh and Üstün, Ahmet and Hooker, Sara},</span>
  <span class="c">year = {2024},</span>
  <span class="c">eprint = {2408.10914},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CL},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/emnlp/" rel="external nofollow noopener" target="_blank">EMNLP</a></abbr></div> <div id="shimabucoro2024llmseellmdo" class="col-sm-8"> <div class="title">LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives</div> <div class="author"> Luísa Shimabucoro, <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a>, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2407.01490" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">shimabucoro2024llmseellmdo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shimabucoro, Luísa and Ruder, Sebastian and Kreutzer, Julia and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2407.01490}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/emnlp/" rel="external nofollow noopener" target="_blank">EMNLP</a></abbr></div> <div id="aakanksha2024multilingualalignmentprismaligning" class="col-sm-8"> <div class="title">The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm</div> <div class="author"> Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.18682" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/aya_redteaming" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">aakanksha2024multilingualalignmentprismaligning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aakanksha and Ahmadian, Arash and Ermis, Beyza and Goldfarb-Tarrant, Seraphina and Kreutzer, Julia and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2406.18682}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="aryabumi2024aya23openweight" class="col-sm-8"> <div class="title">Aya 23: Open Weight Releases to Further Multilingual Progress</div> <div class="author"> Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, <a href="https://kellymarchisio.github.io/" rel="external nofollow noopener" target="_blank">Kelly Marchisio</a>, Max Bartolo, <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a>, Acyr Locatelli, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, Nick Frosst, Aidan Gomez, <a href="https://scholar.google.com/citations?user=eJwbbXEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Phil Blunsom</a>, <em>Marzieh Fadaee</em>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2405.15032" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/CohereLabs/aya-23-35B" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">aryabumi2024aya23openweight</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aya 23: Open Weight Releases to Further Multilingual Progress}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and Dash, Saurabh and Cairuz, David and Lin, Hangyu and Venkitesh, Bharat and Smith, Madeline and Campos, Jon Ander and Tan, Yi Chern and Marchisio, Kelly and Bartolo, Max and Ruder, Sebastian and Locatelli, Acyr and Kreutzer, Julia and Frosst, Nick and Gomez, Aidan and Blunsom, Phil and Fadaee, Marzieh and Üstün, Ahmet and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.15032}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="ahmadian2024basics" class="col-sm-8"> <div class="title">Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</div> <div class="author"> Arash Ahmadian, Chris Cremer, Matthias Gallé, <em>Marzieh Fadaee</em>, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.14740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">ahmadian2024basics</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ahmadian, Arash and Cremer, Chris and Gallé, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Üstün, Ahmet and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.14740}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="üstün2024aya" class="col-sm-8"> <div class="title">Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</div> <div class="author"> <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, <a href="https://scholar.google.com/citations?user=eJwbbXEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Phil Blunsom</a>, <a href="https://www.shaynelongpre.com/" rel="external nofollow noopener" target="_blank">Shayne Longpre</a>, Niklas Muennighoff, <em>Marzieh Fadaee</em>, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.07827" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/CohereLabs/aya-101" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">üstün2024aya</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Üstün, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre, Shayne and Muennighoff, Niklas and Fadaee, Marzieh and Kreutzer, Julia and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.07827}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{Best paper award}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="singh2024aya" class="col-sm-8"> <div class="title">Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning</div> <div class="author"> Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, <a href="https://www.ruder.io/" rel="external nofollow noopener" target="_blank">Sebastian Ruder</a>, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.06619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/aya_dataset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2024aya</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Shivalika and Vargus, Freddie and Dsouza, Daniel and Karlsson, Börje F. and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and OMahony, Laura and Zhang, Mike and Hettiarachchi, Ramith and Wilson, Joseph and Machado, Marina and Souza Moura, Luisa and Krzemiński, Dominik and Fadaei, Hakimeh and Ergün, Irem and Okoh, Ifeoma and Alaagib, Aisha and Mudannayake, Oshan and Alyafeai, Zaid and Chien, Vu Minh and Ruder, Sebastian and Guthikonda, Surya and Alghamdi, Emad A. and Gehrmann, Sebastian and Muennighoff, Niklas and Bartolo, Max and Kreutzer, Julia and Üstün, Ahmet and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.06619}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Neurips</abbr></div> <div id="boubdir2023elo" class="col-sm-8"> <div class="title">Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</div> <div class="author"> <a href="https://twitter.com/mellem_boo" rel="external nofollow noopener" target="_blank">Meriem Boubdir</a>, <a href="https://eddotman.github.io/" rel="external nofollow noopener" target="_blank">Edward Kim</a>, Beyza Ermis, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2311.17295" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/elo_neurips24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">boubdir2023elo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Elo Uncovered: Robustness and Best Practices in Language Model Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Boubdir, Meriem and Kim, Edward and Ermis, Beyza and Hooker, Sara and Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.17295}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="boubdir2023prompts" class="col-sm-8"> <div class="title">Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation</div> <div class="author"> <a href="https://twitter.com/mellem_boo" rel="external nofollow noopener" target="_blank">Meriem Boubdir</a>, <a href="https://eddotman.github.io/" rel="external nofollow noopener" target="_blank">Edward Kim</a>, Beyza Ermis, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2310.14424" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">boubdir2023prompts</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Boubdir, Meriem and Kim, Edward and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.14424}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="marion2023more" class="col-sm-8"> <div class="title">When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale</div> <div class="author"> <a href="https://maxisawesome.github.io/" rel="external nofollow noopener" target="_blank">Max Marion</a>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, <a href="https://luizapozzobon.github.io/" rel="external nofollow noopener" target="_blank">Luiza Pozzobon</a>, <a href="https://w4ngatang.github.io/" rel="external nofollow noopener" target="_blank">Alex Wang</a>, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.04564" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">marion2023more</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marion, Max and Üstün, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.04564}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="https://doi.org/10.48550/arxiv.2301.01820" class="col-sm-8"> <div class="title">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</div> <div class="author"> Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, Jakub Zavrel, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2301.01820" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval},</span>
  <span class="c">author = {Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},</span>
  <span class="c">year = {2023},</span>
  <span class="c">publisher = {arXiv},</span>
  <span class="c">doi = {10.48550/ARXIV.2301.01820},</span>
  <span class="c">copyright = {Creative Commons Attribution 4.0 International},</span>
  <span class="c">keywords = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="https://doi.org/10.48550/arxiv.2212.06121" class="col-sm-8"> <div class="title">In Defense of Cross-Encoders for Zero-Shot Retrieval</div> <div class="author"> Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2212.06121" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Bi-encoders and cross-encoders are widely used in many state-of-the-art retrieval pipelines. In this work we study the generalization ability of these two types of architectures on a wide range of parameter count on both in-domain and out-of-domain scenarios. We find that the number of parameters and early query-document interactions of cross-encoders play a significant role in the generalization ability of retrieval models. Our experiments show that increasing model size results in marginal gains on in-domain test sets, but much larger gains in new domains never seen during fine-tuning. Furthermore, we show that cross-encoders largely outperform bi-encoders of similar size in several tasks. In the BEIR benchmark, our largest cross-encoder surpasses a state-of-the-art bi-encoder by more than 4 average points. Finally, we show that using bi-encoders as first-stage retrievers provides no gains in comparison to a simpler retriever such as BM25 on out-of-domain tasks. The code is available at https://github.com/guilhermemr04/scaling-zero-shot-retrieval</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">author = {Rosa, Guilherme and Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},</span>
  <span class="c">year = {2022},</span>
  <span class="c">publisher = {arXiv},</span>
  <span class="c">doi = {10.48550/ARXIV.2212.06121},</span>
  <span class="c">copyright = {arXiv.org perpetual, non-exclusive license},</span>
  <span class="c">keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://sigir.org/" rel="external nofollow noopener" target="_blank">SIGIR</a></abbr></div> <div id="bonifacio2022inpars" class="col-sm-8"> <div class="title">InPars: Data Augmentation for Information Retrieval using Large Language Models</div> <div class="author"> Luiz Henrique Bonifacio, Hugo Abonizio, <em>Marzieh Fadaee</em>, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> <em>In SIGIR</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2202.05144" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonifacio2022inpars</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InPars: Data Augmentation for Information Retrieval using Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Henrique Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGIR}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2202.05144}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{SIGIR}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="https://doi.org/10.48550/arxiv.2206.02873" class="col-sm-8"> <div class="title">No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval</div> <div class="author"> Guilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> <em>In arXiv</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2206.02873" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Recent work has shown that small distilled language models are strong competitors to models that are orders of magnitude larger and slower in a wide range of information retrieval tasks. This has made distilled and dense models, due to latency constraints, the go-to choice for deployment in real-world retrieval applications. In this work, we question this practice by showing that the number of parameters and early query-document interaction play a significant role in the generalization ability of retrieval models. Our experiments show that increasing model size results in marginal gains on in-domain test sets, but much larger gains in new domains never seen during fine-tuning. Furthermore, we show that rerankers largely outperform dense ones of similar size in several tasks. Our largest reranker reaches the state of the art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the previous state of the art by 3 average points. Finally, we confirm that in-domain effectiveness is not a good indicator of zero-shot effectiveness. Code is available at this https URL</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},</span>
  <span class="c">author = {Rosa, Guilherme Moraes and Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},</span>
  <span class="c">year = {2022},</span>
  <span class="c">booktitle = {arXiv},</span>
  <span class="c">doi = {10.48550/ARXIV.2206.02873},</span>
  <span class="c">copyright = {arXiv.org perpetual, non-exclusive license},</span>
  <span class="c">keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <hr> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="https://doi.org/10.48550/arxiv.2108.13897" class="col-sm-8"> <div class="title">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset</div> <div class="author"> Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, <em>Marzieh Fadaee</em>, Roberto Lotufo, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> <em>In arXiv</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2108.13897" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/unicamp-dl/mmarco" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="abstract hidden"> <p>The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by fine-tuning monolingual and multilingual re-ranking models, as well as a dense multilingual model on this dataset. Experimental results demonstrate that multilingual models fine-tuned on our translated dataset achieve superior effectiveness to models fine-tuned on the original English version alone. Our distilled multilingual re-ranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and fine-tuned models are available at link.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset},</span>
  <span class="c">author = {Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo Queiroz and Campiotti, Israel and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},</span>
  <span class="c">year = {2021},</span>
  <span class="c">booktitle = {arXiv},</span>
  <span class="c">doi = {10.48550/ARXIV.2108.13897},</span>
  <span class="c">copyright = {arXiv.org perpetual, non-exclusive license},</span>
  <span class="c">keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">bib = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2020</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Thesis</abbr></div> <div id="mybook" class="col-sm-8"> <div class="title">Understanding and Enhancing the Use of Context for Machine Translation</div> <div class="author"> <em>Marzieh Fadaee</em> </div> <div class="periodical"> Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dare.uva.nl/search?identifier=422122ad-f9d6-4952-9012-fcde9a820773" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Neural networks learn patterns from data to solve complex problems. To understand and infer meaning in language, neural models have to learn complicated nuances. Meaning is often determined from context. With context, languages allow meaning to be conveyed even when the specific words used are not known by the reader. To model this learning process, a system has to learn from a few instances in context and be able to generalize well to unseen cases. In this thesis, we focus on understanding certain potentials of contexts in neural models and design augmentation models to benefit from them. We focus on machine translation as an important instance of the more general language understanding problem. This task accentuates the value of capturing nuances of language and the necessity of generalization from few observations. The main problem we study in this thesis is what neural machine translation models learn from data and how we can devise more focused contexts to enhance this learning. Looking more in-depth into the role of context and the impact of data on learning models is essential to advance the Natural Language Processing (NLP) field. Understanding the importance of data in the learning process and how neural network models interact with and benefit from data can help develop more accurate NLP systems. Moreover, it helps highlight the vulnerabilities of current neural networks and provides insights into designing more robust models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@book</span><span class="p">{</span><span class="nl">mybook</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding and Enhancing the Use of Context for Machine Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{University of Amsterdam}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9789464210590}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dare.uva.nl/search?identifier=422122ad-f9d6-4952-9012-fcde9a820773}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/ws/" rel="external nofollow noopener" target="_blank">SDP</a></abbr></div> <div id="fadaee-etal-2020-new" class="col-sm-8"> <div class="title">A New Neural Search and Insights Platform for Navigating and Organizing AI Research</div> <div class="author"> <em>Marzieh Fadaee</em>, Olga Gureenkova, Fernando Rejon Barrera, Carsten Schnober, Wouter Weerkamp, and Jakub Zavrel</div> <div class="periodical"> <em>In Proceedings of the First Workshop on Scholarly Document Processing</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.aclweb.org/anthology/2020.sdp-1.23.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover and organize relevant literature. The system provides search at multiple levels of textual granularity, from sentences to aggregations across documents, both in natural language and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, question answering, search, analytics, expert search, and recommendations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-etal-2020-new</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A New Neural Search and Insights Platform for Navigating and Organizing {AI} Research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Gureenkova, Olga and Rejon Barrera, Fernando and Schnober, Carsten and Weerkamp, Wouter and Zavrel, Jakub}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the First Workshop on Scholarly Document Processing}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{207--213}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/2020.sdp-1.23.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/ws/" rel="external nofollow noopener" target="_blank">NGT</a></abbr></div> <div id="fadaee-monz-2020-unreasonable" class="col-sm-8"> <div class="title">The Unreasonable Volatility of Neural Machine Translation Models</div> <div class="author"> <em>Marzieh Fadaee</em>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the Fourth Workshop on Neural Generation and Translation</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.aclweb.org/anthology/2020.ngt-1.10.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where \textitunexpected changes happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26% and 19% of sentence variations, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-monz-2020-unreasonable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Unreasonable Volatility of Neural Machine Translation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Fourth Workshop on Neural Generation and Translation}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{88--96}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/2020.ngt-1.10.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/emnlp/" rel="external nofollow noopener" target="_blank">EMNLP</a></abbr></div> <div id="D18-1040" class="col-sm-8"> <div class="title">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</div> <div class="author"> <em>Marzieh Fadaee</em>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://aclweb.org/anthology/D18-1040.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">D18-1040</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Brussels, Belgium}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{436--446}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/D18-1040.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/lrec/" rel="external nofollow noopener" target="_blank">LREC</a></abbr></div> <div id="2018arXiv180204681F" class="col-sm-8"> <div class="title">Examining the Tip of the Iceberg: A Data Set for Idiom Translation</div> <div class="author"> <em>Marzieh Fadaee</em>, <a href="http://www.cs.rug.nl/~bisazza/" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.aclweb.org/anthology/L18-1148.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Neural Machine Translation (NMT) has been widely used in recent years with significant improvements for many language pairs. Although state-of-the-art NMT systems are generating progressively better translations, idiom translation remains one of the open challenges in this field. Idioms, a category of multiword expressions, are an interesting language phenomenon where the overall meaning of the expression cannot be composed from the meanings of its parts. A first important challenge is the lack of dedicated data sets for learning and evaluating idiom translation. In this paper we address this problem by creating the first large-scale data set for idiom translation. Our data set is automatically extracted from a widely used German↔English translation corpus and includes, for each language direction, a targeted evaluation set where all sentences contain idioms and a regular training corpus where sentences including idioms are marked. We release this data set and use it to perform preliminary NMT experiments as the first step towards better idiom translation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2018arXiv180204681F</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Examining the Tip of the Iceberg: A Data Set for Idiom Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association (ELRA)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Miyazaki, Japan}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/L18-1148.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2017</h2> <hr> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="fadaee-bisazza-monz:2017:Short2" class="col-sm-8"> <div class="title">Data Augmentation for Low-Resource Neural Machine Translation</div> <div class="author"> <em>Marzieh Fadaee</em>, <a href="http://www.cs.rug.nl/~bisazza/" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://aclweb.org/anthology/P17-2090.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-bisazza-monz:2017:Short2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Augmentation for Low-Resource Neural Machine Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{567--573}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/P17-2090.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="fadaee-bisazza-monz:2017:Short1" class="col-sm-8"> <div class="title">Learning Topic-Sensitive Word Representations</div> <div class="author"> <em>Marzieh Fadaee</em>, <a href="http://www.cs.rug.nl/~bisazza/" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://aclweb.org/anthology/P17-2070.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-bisazza-monz:2017:Short1</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Topic-Sensitive Word Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{441--447}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/P17-2070.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2013</h2> <hr> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CICLING</abbr></div> <div id="fadaee2013automatic" class="col-sm-8"> <div class="title">Automatic WordNet Construction Using Markov Chain Monte Carlo</div> <div class="author"> <em>Marzieh Fadaee</em>, <a href="https://hamidreza-ghader.github.io/" rel="external nofollow noopener" target="_blank">Hamidreza Ghader</a>, <a href="https://ece.ut.ac.ir/en/~hfaili" rel="external nofollow noopener" target="_blank">Heshaam Faili</a>, and <a href="https://ece.ut.ac.ir/en/~shakery" rel="external nofollow noopener" target="_blank">Azadeh Shakery</a> </div> <div class="periodical"> <em>Polibits</em>, Jul 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://staff.fnwi.uva.nl/m.fadaee/files/pwn.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>WordNet is used extensively as a major lexical resource in information retrieval tasks. However, the qualities of existing Persian WordNets are far from perfect. They are either constructed manually which limits the coverage of Persian words, or automatically which results in unsatisfactory precision. This paper presents a fully-automated approach for constructing a Persian WordNet: A Bayesian Model with Markov chain Monte Carlo (MCMC) estimation. We model the problem of constructing a Persian WordNet by estimating the probability of assigning senses (synsets) to Persian words. By applying MCMC techniques in estimating these probabilities, we integrate prior knowledge in the estimation and use the expected value of generated samples to give the final estimates. This ensures great performance improvement comparing with Maximum-Likelihood and Expectation-Maximization methods. Our acquired WordNet has a precision of 90.46% which is a considerable improvement in comparison with automatically-built WordNets in Persian.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">fadaee2013automatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic WordNet Construction Using Markov Chain Monte Carlo}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Ghader, Hamidreza and Faili, Heshaam and Shakery, Azadeh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Polibits}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Instituto Polit{\'e}cnico Nacional, Centro de Innovaci{\'o}n y Desarrollo Tecnol{\'o}gico en C{\'o}mputo}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13--22}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://staff.fnwi.uva.nl/m.fadaee/files/pwn.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Marzieh Fadaee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?40a752d5efea2d0dae8b9d1dee79d33d"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>