@misc{dash2025ayavisionadvancingfrontier,
      title={Aya Vision: Advancing the Frontier of Multilingual Multimodality}, 
      author={Saurabh Dash and Yiyang Nan and John Dang and Arash Ahmadian and Shivalika Singh and Madeline Smith and Bharat Venkitesh and Vlad Shmyhlo and Viraat Aryabumi and Walter Beller-Morales and Jeremy Pekmez and Jason Ozuzu and Pierre Richemond and Acyr Locatelli and Nick Frosst and Phil Blunsom and Aidan Gomez and Ivan Zhang and Marzieh Fadaee and Manoj Govindassamy and Sudip Roy and Matthias Gallé and Beyza Ermis and Ahmet Üstün and Sara Hooker},
      year={2025},
      eprint={2505.08751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.08751}, 
	  abbr={ArXiv},
	  bibtex_show={true},
	  data={https://huggingface.co/collections/CohereLabs/cohere-labs-aya-vision-67c4ccd395ca064308ee1484},
	  model={https://huggingface.co/CohereLabs/aya-vision-32b}
}

@misc{singh2025leaderboardillusion,
      title={The Leaderboard Illusion}, 
      author={Shivalika Singh and Yiyang Nan and Alex Wang and Daniel D'Souza and Sayash Kapoor and Ahmet Üstün and Sanmi Koyejo and Yuntian Deng and Shayne Longpre and Noah A. Smith and Beyza Ermis and Marzieh Fadaee and Sara Hooker},
      year={2025},
      eprint={2504.20879},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.20879}, 
	  abbr={ArXiv},
	  selected     = {true},
	  bibtex_show={true},
	  website={https://cohere.com/research/lmarena}
}

@misc{shimabucoro2025posttrainersguidemultilingualtraining,
      title={A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics}, 
      author={Luisa Shimabucoro and Ahmet Ustun and Marzieh Fadaee and Sebastian Ruder},
      year={2025},
      eprint={2504.16677},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.16677}, 
	  abbr={ArXiv},
	  bibtex_show={true},
}

@misc{kreutzer2025dejavumultilingualllm,
      title={D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation}, 
      author={Julia Kreutzer and Eleftheria Briakou and Sweta Agrawal and Marzieh Fadaee and Kocmi Tom},
      year={2025},
      eprint={2504.11829},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.11829}, 
	  abbr={ArXiv},
	  bibtex_show={true},
}

@misc{salazar2025kaleidoscopeinlanguageexamsmassively,
      title={Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation}, 
      author={Israfel Salazar and Manuel Fernández Burda and Shayekh Bin Islam and Arshia Soltani Moakhar and Shivalika Singh and Fabian Farestam and Angelika Romanou and Danylo Boiko and Dipika Khullar and Mike Zhang and Dominik Krzemiński and Jekaterina Novikova and Luísa Shimabucoro and Joseph Marvin Imperial and Rishabh Maheshwary and Sharad Duwal and Alfonso Amayuelas and Swati Rajwal and Jebish Purbey and Ahmed Ruby and Nicholas Popovič and Marek Suppa and Azmine Toushik Wasi and Ram Mohan Rao Kadiyala and Olga Tsymboi and Maksim Kostritsya and Bardia Soltani Moakhar and Gabriel da Costa Merlin and Otávio Ferracioli Coletti and Maral Jabbari Shiviari and MohammadAmin farahani fard and Silvia Fernandez and María Grandury and Dmitry Abulkhanov and Drishti Sharma and Andre Guarnier De Mitri and Leticia Bossatto Marchezi and Setayesh Heydari and Johan Obando-Ceron and Nazar Kohut and Beyza Ermis and Desmond Elliott and Enzo Ferrante and Sara Hooker and Marzieh Fadaee},
      year={2025},
      eprint={2504.07072},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.07072}, 
	  abbr={ArXiv},
	  selected     = {true},
	  bibtex_show={true},
	  data={https://huggingface.co/datasets/CohereLabs/kaleidoscope}
}

@misc{cohere2025commandaenterprisereadylarge,
      title={Command A: An Enterprise-Ready Large Language Model}, 
      author={Team Cohere and Aakanksha and Arash Ahmadian and Marwan Ahmed and Jay Alammar and Milad Alizadeh and Yazeed Alnumay and Sophia Althammer and Arkady Arkhangorodsky and Viraat Aryabumi and Dennis Aumiller and Raphaël Avalos and Zahara Aviv and Sammie Bae and Saurabh Baji and Alexandre Barbet and Max Bartolo and Björn Bebensee and Neeral Beladia and Walter Beller-Morales and Alexandre Bérard and Andrew Berneshawi and Anna Bialas and Phil Blunsom and Matt Bobkin and Adi Bongale and Sam Braun and Maxime Brunet and Samuel Cahyawijaya and David Cairuz and Jon Ander Campos and Cassie Cao and Kris Cao and Roman Castagné and Julián Cendrero and Leila Chan Currie and Yash Chandak and Diane Chang and Giannis Chatziveroglou and Hongyu Chen and Claire Cheng and Alexis Chevalier and Justin T. Chiu and Eugene Cho and Eugene Choi and Eujeong Choi and Tim Chung and Volkan Cirik and Ana Cismaru and Pierre Clavier and Henry Conklin and Lucas Crawhall-Stein and Devon Crouse and Andres Felipe Cruz-Salinas and Ben Cyrus and Daniel D'souza and Hugo Dalla-Torre and John Dang and William Darling and Omar Darwiche Domingues and Saurabh Dash and Antoine Debugne and Théo Dehaze and Shaan Desai and Joan Devassy and Rishit Dholakia and Kyle Duffy and Ali Edalati and Ace Eldeib and Abdullah Elkady and Sarah Elsharkawy and Irem Ergün and Beyza Ermis and Marzieh Fadaee and Boyu Fan and Lucas Fayoux and Yannis Flet-Berliac and Nick Frosst and Matthias Gallé and Wojciech Galuba and Utsav Garg and Matthieu Geist and Mohammad Gheshlaghi Azar and Ellen Gilsenan-McMahon and Seraphina Goldfarb-Tarrant and Tomas Goldsack and Aidan Gomez and Victor Machado Gonzaga and Nithya Govindarajan and Manoj Govindassamy and Nathan Grinsztajn and Nikolas Gritsch and Patrick Gu and Shangmin Guo and Kilian Haefeli and Rod Hajjar and Tim Hawes and Jingyi He and Sebastian Hofstätter and Sungjin Hong and Sara Hooker and Tom Hosking and Stephanie Howe and Eric Hu and Renjie Huang and Hemant Jain and Ritika Jain and Nick Jakobi and Madeline Jenkins and JJ Jordan and Dhruti Joshi and Jason Jung and Trushant Kalyanpur and Siddhartha Rao Kamalakara and Julia Kedrzycki and Gokce Keskin and Edward Kim and Joon Kim and Wei-Yin Ko and Tom Kocmi and Michael Kozakov and Wojciech Kryściński and Arnav Kumar Jain and Komal Kumar Teru and Sander Land and Michael Lasby and Olivia Lasche and Justin Lee and Patrick Lewis and Jeffrey Li and Jonathan Li and Hangyu Lin and Acyr Locatelli and Kevin Luong and Raymond Ma and Lukáš Mach and Marina Machado and Joanne Magbitang and Brenda Malacara Lopez and Aryan Mann and Kelly Marchisio and Olivia Markham and Alexandre Matton and Alex McKinney and Dominic McLoughlin and Jozef Mokry and Adrien Morisot and Autumn Moulder and Harry Moynehan and Maximilian Mozes and Vivek Muppalla and Lidiya Murakhovska and Hemangani Nagarajan and Alekhya Nandula and Hisham Nasir and Shauna Nehra and Josh Netto-Rosen and Daniel Ohashi and James Owers-Bardsley and Jason Ozuzu and Dennis Padilla and Gloria Park and Sam Passaglia and Jeremy Pekmez and Laura Penstone and Aleksandra Piktus and Case Ploeg and Andrew Poulton and Youran Qi and Shubha Raghvendra and Miguel Ramos and Ekagra Ranjan and Pierre Richemond and Cécile Robert-Michon and Aurélien Rodriguez and Sudip Roy and Sebastian Ruder and Laura Ruis and Louise Rust and Anubhav Sachan and Alejandro Salamanca and Kailash Karthik Saravanakumar and Isha Satyakam and Alice Schoenauer Sebag and Priyanka Sen and Sholeh Sepehri and Preethi Seshadri and Ye Shen and Tom Sherborne and Sylvie Shang Shi and Sanal Shivaprasad and Vladyslav Shmyhlo and Anirudh Shrinivason and Inna Shteinbuk and Amir Shukayev and Mathieu Simard and Ella Snyder and Ava Spataru and Victoria Spooner and Trisha Starostina and Florian Strub and Yixuan Su and Jimin Sun and Dwarak Talupuru and Eugene Tarassov and Elena Tommasone and Jennifer Tracey and Billy Trend and Evren Tumer and Ahmet Üstün and Bharat Venkitesh and David Venuto and Pat Verga and Maxime Voisin and Alex Wang and Donglu Wang and Shijian Wang and Edmond Wen and Naomi White and Jesse Willman and Marysia Winkels and Chen Xia and Jessica Xie and Minjie Xu and Bowen Yang and Tan Yi-Chern and Ivan Zhang and Zhenyu Zhao and Zhoujie Zhao},
      year={2025},
      eprint={2504.00698},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00698}, 
	  abbr={ArXiv},
	  bibtex_show={true},
	  model={https://huggingface.co/CohereLabs/c4ai-command-a-03-2025}
}

@misc{rakotonirina2025toolsteammatesevaluatingllms,
      title={From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions}, 
      author={Nathanaël Carraz Rakotonirina and Mohammed Hamdy and Jon Ander Campos and Lucas Weber and Alberto Testoni and Marzieh Fadaee and Sandro Pezzelle and Marco Del Tredici},
      year={2025},
      eprint={2502.13791},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.13791}, 
	  abbr={ACL},
	  bibtex_show={true},
	  code={https://github.com/Cohere-Labs-Community/MemoryCode},
	  data={https://huggingface.co/datasets/CohereLabsCommunity/memorycode}
}

@misc{baack2025bestpracticesopendatasets,
      title={Towards Best Practices for Open Datasets for LLM Training}, 
      author={Stefan Baack and Stella Biderman and Kasia Odrozek and Aviya Skowron and Ayah Bdeir and Jillian Bommarito and Jennifer Ding and Maximilian Gahntz and Paul Keller and Pierre-Carl Langlais and Greg Lindahl and Sebastian Majstorovic and Nik Marda and Guilherme Penedo and Maarten Van Segbroeck and Jennifer Wang and Leandro von Werra and Mitchell Baker and Julie Belião and Kasia Chmielinski and Marzieh Fadaee and Lisa Gutermuth and Hynek Kydlíček and Greg Leppert and EM Lewis-Jong and Solana Larsen and Shayne Longpre and Angela Oduor Lungati and Cullen Miller and Victor Miller and Max Ryabinin and Kathleen Siminyu and Andrew Strait and Mark Surman and Anna Tumadóttir and Maurice Weber and Rebecca Weiss and Lee White and Thomas Wolf},
      year={2025},
      eprint={2501.08365},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2501.08365}, 
	  bibtex_show={true},

}

@misc{dang2024ayaexpansecombiningresearch,
      title={Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier}, 
      author={John Dang and Shivalika Singh and Daniel D'souza and Arash Ahmadian and Alejandro Salamanca and Madeline Smith and Aidan Peppin and Sungjin Hong and Manoj Govindassamy and Terrence Zhao and Sandra Kublik and Meor Amer and Viraat Aryabumi and Jon Ander Campos and Yi-Chern Tan and Tom Kocmi and Florian Strub and Nathan Grinsztajn and Yannis Flet-Berliac and Acyr Locatelli and Hangyu Lin and Dwarak Talupuru and Bharat Venkitesh and David Cairuz and Bowen Yang and Tim Chung and Wei-Yin Ko and Sylvie Shang Shi and Amir Shukayev and Sammie Bae and Aleksandra Piktus and Roman Castagné and Felipe Cruz-Salinas and Eddie Kim and Lucas Crawhall-Stein and Adrien Morisot and Sudip Roy and Phil Blunsom and Ivan Zhang and Aidan Gomez and Nick Frosst and Marzieh Fadaee and Beyza Ermis and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2412.04261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.04261}, 
	  abbr={ArXiv},
	  bibtex_show={true},
	  model={https://huggingface.co/CohereLabs/aya-expanse-32b},
	  supp={https://huggingface.co/collections/CohereLabs/cohere-labs-aya-expanse-671a83d6b2c07c692beab3c3}
}

@misc{singh2024globalmmluunderstandingaddressing,
      title={Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation}, 
      author={Shivalika Singh and Angelika Romanou and Clémentine Fourrier and David I. Adelani and Jian Gang Ngui and Daniel Vila-Suero and Peerat Limkonchotiwat and Kelly Marchisio and Wei Qi Leong and Yosephine Susanto and Raymond Ng and Shayne Longpre and Wei-Yin Ko and Madeline Smith and Antoine Bosselut and Alice Oh and Andre F. T. Martins and Leshem Choshen and Daphne Ippolito and Enzo Ferrante and Marzieh Fadaee and Beyza Ermis and Sara Hooker},
      year={2024},
      eprint={2412.03304},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.03304}, 
	  abbr={ACL},
	  bibtex_show={true},
	  data={https://huggingface.co/datasets/CohereLabs/Global-MMLU}
}

@misc{romanou2024includeevaluatingmultilinguallanguage,
      title={INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge}, 
      author={Angelika Romanou and Negar Foroutan and Anna Sotnikova and Zeming Chen and Sree Harsha Nelaturu and Shivalika Singh and Rishabh Maheshwary and Micol Altomare and Mohamed A. Haggag and Snegha A and Alfonso Amayuelas and Azril Hafizi Amirudin and Viraat Aryabumi and Danylo Boiko and Michael Chang and Jenny Chim and Gal Cohen and Aditya Kumar Dalmia and Abraham Diress and Sharad Duwal and Daniil Dzenhaliou and Daniel Fernando Erazo Florez and Fabian Farestam and Joseph Marvin Imperial and Shayekh Bin Islam and Perttu Isotalo and Maral Jabbarishiviari and Börje F. Karlsson and Eldar Khalilov and Christopher Klamm and Fajri Koto and Dominik Krzemiński and Gabriel Adriano de Melo and Syrielle Montariol and Yiyang Nan and Joel Niklaus and Jekaterina Novikova and Johan Samir Obando Ceron and Debjit Paul and Esther Ploeger and Jebish Purbey and Swati Rajwal and Selvan Sunitha Ravi and Sara Rydell and Roshan Santhosh and Drishti Sharma and Marjana Prifti Skenduli and Arshia Soltani Moakhar and Bardia Soltani Moakhar and Ran Tamir and Ayush Kumar Tarun and Azmine Toushik Wasi and Thenuka Ovin Weerasinghe and Serhan Yilmaz and Mike Zhang and Imanol Schlag and Marzieh Fadaee and Sara Hooker and Antoine Bosselut},
      year={2024},
      eprint={2411.19799},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.19799}, 
	  abbr={ICLR},
	  bibtex_show={true},
	  selected     = {true},
	  data={https://huggingface.co/datasets/CohereLabs/include-base-44},
	  poster={romanou_iclr_2025_poster.pdf}
}

@misc{gureja2024mrewardbenchevaluatingrewardmodels,
      title={M-RewardBench: Evaluating Reward Models in Multilingual Settings}, 
      author={Srishti Gureja and Lester James V. Miranda and Shayekh Bin Islam and Rishabh Maheshwary and Drishti Sharma and Gusti Winata and Nathan Lambert and Sebastian Ruder and Sara Hooker and Marzieh Fadaee},
      year={2024},
      eprint={2410.15522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.15522}, 
	  abbr={ACL},
	  bibtex_show={true},
	  code={https://github.com/Cohere-Labs-Community/m-rewardbench},
	  data={https://huggingface.co/datasets/CohereLabsCommunity/multilingual-reward-bench}
}

@misc{aakanksha2024mixdatamergemodels,
      title={Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning}, 
      author={Aakanksha and Arash Ahmadian and Seraphina Goldfarb-Tarrant and Beyza Ermis and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2410.10801},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10801}, 
	  abbr={Neurips Wokshop},
	  bibtex_show={true},
}

@misc{yu2024diversifyconquerdiversitycentricdata,
      title={Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement}, 
      author={Simon Yu and Liangyu Chen and Sara Ahmadian and Marzieh Fadaee},
      year={2024},
      eprint={2409.11378},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.11378}, 
	  abbr={ArXiv},
	  selected     = {true},
	  bibtex_show={true},
	  code={https://github.com/Cohere-Labs-Community/iterative-data-selection}
}
@misc{aryabumi2024codecodeexploringimpact,
      title={To Code, or Not To Code? Exploring Impact of Code in Pre-training}, 
      author={Viraat Aryabumi and Yixuan Su and Raymond Ma and Adrien Morisot and Ivan Zhang and Acyr Locatelli and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2408.10914},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.10914}, 
	  abbr={ICLR},
	  bibtex_show={true},
	  poster={ICLR 2025 - Poster Landscape - Code in pretraining.png}
}

@misc{shimabucoro2024llmseellmdo,
      title={LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives}, 
      author={Luísa Shimabucoro and Sebastian Ruder and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2407.01490},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01490}, 
	  abbr={EMNLP},
	  bibtex_show={true},
}
@misc{aakanksha2024multilingualalignmentprismaligning,
      title={The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm}, 
      author={Aakanksha and Arash Ahmadian and Beyza Ermis and Seraphina Goldfarb-Tarrant and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2406.18682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18682}, 
	  abbr={EMNLP},
	  bibtex_show={true},
	  data={https://huggingface.co/datasets/CohereLabs/aya_redteaming}
}

@misc{aryabumi2024aya23openweight,
      title={Aya 23: Open Weight Releases to Further Multilingual Progress}, 
      author={Viraat Aryabumi and John Dang and Dwarak Talupuru and Saurabh Dash and David Cairuz and Hangyu Lin and Bharat Venkitesh and Madeline Smith and Jon Ander Campos and Yi Chern Tan and Kelly Marchisio and Max Bartolo and Sebastian Ruder and Acyr Locatelli and Julia Kreutzer and Nick Frosst and Aidan Gomez and Phil Blunsom and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2405.15032},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.15032}, 
	  abbr={ArXiv},
	  bibtex_show={true},
	  model={https://huggingface.co/CohereLabs/aya-23-35B}
}

@misc{ahmadian2024basics,
      title={Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs}, 
      author={Ahmadian, Arash and Cremer, Chris and Gallé, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Üstün, Ahmet and Hooker, Sara},
      year={2024},
      eprint={2402.14740},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
	  url={https://arxiv.org/abs/2402.14740},
	  abbr={ACL},
	  bibtex_show={true},
}
@misc{üstün2024aya,
      title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}, 
      author={Üstün, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre, Shayne and Muennighoff, Niklas and Fadaee, Marzieh and Kreutzer, Julia and Hooker, Sara},
      year={2024},
      eprint={2402.07827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  url={https://arxiv.org/abs/2402.07827},
	  abbr={ACL},
	  bibtex_show={true},
	  award={Best paper award},
	  model={https://huggingface.co/CohereLabs/aya-101}

}
@misc{singh2024aya,
      title={Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning}, 
      author={Singh, Shivalika and Vargus, Freddie and Dsouza, Daniel and Karlsson, Börje F. and Mahendiran, Abinaya and Ko, Wei-Yin and Shandilya, Herumb and Patel, Jay and Mataciunas, Deividas and OMahony, Laura and Zhang, Mike and Hettiarachchi, Ramith and Wilson, Joseph and Machado, Marina and Souza Moura, Luisa and Krzemiński, Dominik and Fadaei, Hakimeh and Ergün, Irem and Okoh, Ifeoma and Alaagib, Aisha and Mudannayake, Oshan and Alyafeai, Zaid and Chien, Vu Minh and Ruder, Sebastian and Guthikonda, Surya and Alghamdi, Emad A. and Gehrmann, Sebastian and Muennighoff, Niklas and Bartolo, Max and Kreutzer, Julia and Üstün, Ahmet and Fadaee, Marzieh and Hooker, Sara},
      year={2024},
      eprint={2402.06619},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  url={https://arxiv.org/abs/2402.06619},
	  abbr={ACL},
	  bibtex_show={true},
	  data={https://huggingface.co/datasets/CohereLabs/aya_dataset}
}
@misc{boubdir2023elo,
      title={Elo Uncovered: Robustness and Best Practices in Language Model Evaluation}, 
      author={Boubdir, Meriem and Kim, Edward and Ermis, Beyza and Hooker, Sara and Fadaee, Marzieh},
      year={2023},
      eprint={2311.17295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  url={https://arxiv.org/abs/2311.17295},
	  abbr={Neurips},
	  bibtex_show={true},
	  selected     = {true},
	  poster={elo_neurips24_poster.pdf},
}
@misc{boubdir2023prompts,
      title={Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation}, 
      author={Boubdir, Meriem and Kim, Edward and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara},
      year={2023},
      eprint={2310.14424},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  url={https://arxiv.org/abs/2310.14424},
	  abbr={ArXiv},
	  bibtex_show={true},
}

@misc{marion2023more,
      title={When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale}, 
      author={Marion, Max and Üstün, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara},
      year=2023,
      eprint={2309.04564},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  abstract={Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.},
	  selected = {true},
	  url={https://arxiv.org/abs/2309.04564},
	  abbr={ArXiv},
	  bibtex_show={true},

}
@misc{https://doi.org/10.48550/arxiv.2301.01820,
	title        = {InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval},
	author       = {Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},
	year         = 2023,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2301.01820},
	url          = {https://arxiv.org/abs/2301.01820},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	abstract     = {Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu},
	selected     = {true},
	abbr={ArXiv},
	bibtex_show={true},
}
@misc{https://doi.org/10.48550/arxiv.2212.06121,
	title        = {In Defense of Cross-Encoders for Zero-Shot Retrieval},
	author       = {Rosa, Guilherme and Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.06121},
	url          = {https://arxiv.org/abs/2212.06121},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	abbr={ArXiv},
	bibtex_show={true},
	abstract     = {Bi-encoders and cross-encoders are widely used in many state-of-the-art retrieval pipelines. In this work we study the generalization ability of these two types of architectures on a wide range of parameter count on both in-domain and out-of-domain scenarios. We find that the number of parameters and early query-document interactions of cross-encoders play a significant role in the generalization ability of retrieval models. Our experiments show that increasing model size results in marginal gains on in-domain test sets, but much larger gains in new domains never seen during fine-tuning. Furthermore, we show that cross-encoders largely outperform bi-encoders of similar size in several tasks. In the BEIR benchmark, our largest cross-encoder surpasses a state-of-the-art bi-encoder by more than 4 average points. Finally, we show that using bi-encoders as first-stage retrievers provides no gains in comparison to a simpler retriever such as BM25 on out-of-domain tasks. The code is available at https://github.com/guilhermemr04/scaling-zero-shot-retrieval},
}
@inproceedings{bonifacio2022inpars,
	title        = {InPars: Data Augmentation for Information Retrieval using Large Language Models},
	author       = {Henrique Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
	year         = 2022,
	month        = feb,
	booktitle    = {SIGIR},
	eprint       = {2202.05144},
	archiveprefix = {SIGIR},
	abbr         = {SIGIR},
	primaryclass = {cs.CL},
	abstract     = {The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars},
	url          = {https://arxiv.org/pdf/2202.05144},
	bibtex_show={true},
}
@inproceedings{https://doi.org/10.48550/arxiv.2206.02873,
	title        = {No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},
	author       = {Rosa, Guilherme Moraes and Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto, and Nogueira, Rodrigo},
	year         = 2022,
	booktitle    = {arXiv},
	doi          = {10.48550/ARXIV.2206.02873},
	url          = {https://arxiv.org/abs/2206.02873},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
	abstract     = {Recent work has shown that small distilled language models are strong competitors to models that are orders of magnitude larger and slower in a wide range of information retrieval tasks. This has made distilled and dense models, due to latency constraints, the go-to choice for deployment in real-world retrieval applications. In this work, we question this practice by showing that the number of parameters and early query-document interaction play a significant role in the generalization ability of retrieval models. Our experiments show that increasing model size results in marginal gains on in-domain test sets, but much larger gains in new domains never seen during fine-tuning. Furthermore, we show that rerankers largely outperform dense ones of similar size in several tasks. Our largest reranker reaches the state of the art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the previous state of the art by 3 average points. Finally, we confirm that in-domain effectiveness is not a good indicator of zero-shot effectiveness. Code is available at this https URL},
	url          = {https://arxiv.org/abs/2206.02873},
	abbr={ArXiv},
	bibtex_show={true},
}
@inproceedings{https://doi.org/10.48550/arxiv.2108.13897,
	title        = {mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset},
	author       = {Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo Queiroz and Campiotti, Israel and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},
	year         = 2021,
	booktitle    = {arXiv},
	doi          = {10.48550/ARXIV.2108.13897},
	url          = {https://arxiv.org/abs/2108.13897},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	archiveprefix = {arXiv},
	abstract     = {The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by fine-tuning monolingual and multilingual re-ranking models, as well as a dense multilingual model on this dataset. Experimental results demonstrate that multilingual models fine-tuned on our translated dataset achieve superior effectiveness to models fine-tuned on the original English version alone. Our distilled multilingual re-ranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and fine-tuned models are available at link.},
	bib          = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},
	url          = {https://arxiv.org/pdf/2108.13897},
	abbr={ArXiv},
	bibtex_show={true},
	data={https://huggingface.co/datasets/unicamp-dl/mmarco}
}
@book{mybook,
	title        = {Understanding and Enhancing the Use of Context for Machine Translation},
	author       = {Fadaee, Marzieh},
	year         = 2020,
	month        = 10,
	publisher    = {University of Amsterdam},
	isbn         = 9789464210590,
	url          = {https://marziehf.github.io/assets/pdf/Thesis.pdf},
	abbr={Thesis},
	bibtex_show={true},
	abstract     = {Neural networks learn patterns from data to solve complex problems. To understand and infer meaning in language, neural models have to learn complicated nuances. Meaning is often determined from context. With context, languages allow meaning to be conveyed even when the specific words used are not known by the reader. To model this learning process, a system has to learn from a few instances in context and be able to generalize well to unseen cases. In this thesis, we focus on understanding certain potentials of contexts in neural models and design augmentation models to benefit from them. We focus on machine translation as an important instance of the more general language understanding problem. This task accentuates the value of capturing nuances of language and the necessity of generalization from few observations. The main problem we study in this thesis is what neural machine translation models learn from data and how we can devise more focused contexts to enhance this learning. Looking more in-depth into the role of context and the impact of data on learning models is essential to advance the Natural Language Processing (NLP) field. Understanding the importance of data in the learning process and how neural network models interact with and benefit from data can help develop more accurate NLP systems. Moreover, it helps highlight the vulnerabilities of current neural networks and provides insights into designing more robust models.}
}
@inproceedings{fadaee-etal-2020-new,
	title        = {A New Neural Search and Insights Platform for Navigating and Organizing {AI} Research},
	author       = {Fadaee, Marzieh  and Gureenkova, Olga  and Rejon Barrera, Fernando  and Schnober, Carsten  and Weerkamp, Wouter  and Zavrel, Jakub},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the First Workshop on Scholarly Document Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {207--213},
	url          = {https://www.aclweb.org/anthology/2020.sdp-1.23},
	abbr         = {SDP},
	abstract     = {To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover and organize relevant literature. The system provides search at multiple levels of textual granularity, from sentences to aggregations across documents, both in natural language and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, question answering, search, analytics, expert search, and recommendations.},
	url          = {https://www.aclweb.org/anthology/2020.sdp-1.23.pdf},
	bib          = {https://www.aclweb.org/anthology/2020.sdp-1.23.bib},
	bibtex_show={true},
}
@inproceedings{fadaee-monz-2020-unreasonable,
	title        = {The Unreasonable Volatility of Neural Machine Translation Models},
	author       = {Fadaee, Marzieh  and Monz, Christof},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the Fourth Workshop on Neural Generation and Translation},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {88--96},
	url          = {https://www.aclweb.org/anthology/2020.ngt-1.10},
	abbr         = {NGT},
	abstract     = {Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where \textit{unexpected changes} happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26{\%} and 19{\%} of sentence variations, respectively.},
	bib          = {https://www.aclweb.org/anthology/2020.ngt-1.10.bib},
	url          = {https://www.aclweb.org/anthology/2020.ngt-1.10.pdf},
	selected     = {true},
	bibtex_show={true},
}
---
---
@inproceedings{D18-1040,
	title        = {Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation},
	author       = {Fadaee, Marzieh and Monz, Christof},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	location     = {Brussels, Belgium},
	publisher    = {Association for Computational Linguistics},
	pages        = {436--446},
	abbr         = {EMNLP},
	url          = {http://aclweb.org/anthology/D18-1040.pdf},
	abstract     = {Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively},
	bib          = {https://www.aclweb.org/anthology/D18-1040.bib},
	bibtex_show={true},
}
@inproceedings{2018arXiv180204681F,
	title        = {Examining the Tip of the Iceberg: A Data Set for Idiom Translation},
	author       = {Fadaee, Marzieh  and Bisazza, Arianna  and Monz, Christof},
	year         = 2018,
	month        = may,
	booktitle    = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
	publisher    = {European Language Resources Association (ELRA)},
	address      = {Miyazaki, Japan},
	url          = {https://www.aclweb.org/anthology/L18-1148},
	abbr         = {LREC},
	abstract     = {Neural Machine Translation (NMT) has been widely used in recent years with significant improvements for many language pairs. Although state-of-the-art NMT systems are generating progressively better translations, idiom translation remains one of the open challenges in this field. Idioms, a category of multiword expressions, are an interesting language phenomenon where the overall meaning of the expression cannot be composed from the meanings of its parts. A first important challenge is the lack of dedicated data sets for learning and evaluating idiom translation. In this paper we address this problem by creating the first large-scale data set for idiom translation. Our data set is automatically extracted from a widely used German↔English translation corpus and includes, for each language direction, a targeted evaluation set where all sentences contain idioms and a regular training corpus where sentences including idioms are marked. We release this data set and use it to perform preliminary NMT experiments as the first step towards better idiom translation.},
	bib          = {https://www.aclweb.org/anthology/L18-1148.bib},
	url          = {https://www.aclweb.org/anthology/L18-1148.pdf},
	bibtex_show={true},
}
@inproceedings{fadaee-bisazza-monz:2017:Short2,
	title        = {Data Augmentation for Low-Resource Neural Machine Translation},
	author       = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
	year         = 2017,
	month        = {July},
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {567--573},
	abbr         = {ACL},
	abstract     = {The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.},
	url          = {http://aclweb.org/anthology/P17-2090.pdf},
	bib          = {https://www.aclweb.org/anthology/P17-2090.bib},
	selected     = {true},
	bibtex_show={true},
}
@inproceedings{fadaee-bisazza-monz:2017:Short1,
	title        = {Learning Topic-Sensitive Word Representations},
	author       = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
	year         = 2017,
	month        = {July},
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {441--447},
	abbr         = {ACL},
	abstract     = {Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document  we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.},
	url          = {http://aclweb.org/anthology/P17-2070.pdf},
	bib          = {https://www.aclweb.org/anthology/P17-2070.bib},
	bibtex_show={true},
}
@article{fadaee2013automatic,
	title        = {Automatic WordNet Construction Using Markov Chain Monte Carlo},
	author       = {Fadaee, Marzieh and Ghader, Hamidreza and Faili, Heshaam and Shakery, Azadeh},
	year         = 2013,
	journal      = {Polibits},
	publisher    = {Instituto Polit{\'e}cnico Nacional, Centro de Innovaci{\'o}n y Desarrollo Tecnol{\'o}gico en C{\'o}mputo},
	volume       = 47,
	pages        = {13--22},
	abstract     = {WordNet is used extensively as a major lexical resource in information retrieval tasks. However, the qualities of existing Persian WordNets are far from perfect. They are either constructed manually which limits the coverage of Persian words, or automatically which results in unsatisfactory precision. This paper presents a fully-automated approach for constructing a Persian WordNet: A Bayesian Model with Markov chain Monte Carlo (MCMC) estimation. We model the problem of constructing a Persian WordNet by estimating the probability of assigning senses (synsets) to Persian words. By applying MCMC techniques in estimating these probabilities, we integrate prior knowledge in the estimation and use the expected value of generated samples to give the final estimates. This ensures great performance improvement comparing with Maximum-Likelihood and Expectation-Maximization methods. Our acquired WordNet has a precision of 90.46&#37; which is a considerable improvement in comparison with automatically-built WordNets in Persian.},
	url          = {https://staff.fnwi.uva.nl/m.fadaee/files/pwn.pdf},
	bib          = {https://staff.fnwi.uva.nl/m.fadaee/files/pwn.bib},
	abbr={CICLING},
	bibtex_show={true},
}
