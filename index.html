<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Marzieh Fadaee</title> <meta name="author" content="Marzieh Fadaee"> <meta name="description" content="Homepage for Marzieh Fadaee "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://marziehf.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?f9d4e7248e54e0dde2dddcb251b631cc"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Marzieh</span> Fadaee </h1> <p class="desc"><strong>Staff Research Scientist</strong> @ <a href="https://cohere.for.ai/" rel="external nofollow noopener" target="_blank">Cohere Labs</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_moi-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_moi-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_moi-1400.webp"></source> <img src="/assets/img/prof_pic_moi.jpg?1ac321189e6396eb4cb0b9a7cddd39b4" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic_moi.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I’m a research scientist at <a href="https://cohere.com/research" rel="external nofollow noopener" target="_blank">Cohere Labs</a>, the non-profit research lab of <a href="https://cohere.com/" rel="external nofollow noopener" target="_blank">Cohere</a>, working with <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> on complex problems and fundamental research in language understanding. As a scientist, I’m broadly interested in all aspects of natural language understanding, and particularly in multilingual learning, data-conscious learning, robust and scalable models, compositionality, and evaluation.</p> <p>Previously I was the NLP/ML research lead at <a href="https://www.zeta-alpha.com/" rel="external nofollow noopener" target="_blank">Zeta Alpha Vector</a> working on smarter ways to discover and organize knowledge. I did my PhD at the <a href="https://ltl.science.uva.nl/" rel="external nofollow noopener" target="_blank">Language Technology Lab</a> (originally part of the <a href="https://irlab.science.uva.nl/" rel="external nofollow noopener" target="_blank">ILPS group</a>), University of Amsterdam, working on developing models to understand and utilize interesting phenomena in the data. During my PhD I was advised by <a href="https://staff.fnwi.uva.nl/c.monz/index.html" rel="external nofollow noopener" target="_blank">Christof Monz</a> and <a href="http://www.cs.rug.nl/~bisazza/index.html" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>. I received my B.Sc. from <a href="http://www.en.sharif.edu/" rel="external nofollow noopener" target="_blank">Sharif University</a> majoring in Computer Engineering and M.Sc. from <a href="https://ut.ac.ir/en" rel="external nofollow noopener" target="_blank">University of Tehran</a> majoring in Artificial Intelligence.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 18, 2025</th> <td> Three papers accepted to ACL main track: <a href="https://arxiv.org/abs/2502.13791" target="blank" rel="external nofollow noopener">From Tools to Teammates</a>, <a href="https://arxiv.org/abs/2412.03304" target="blank" rel="external nofollow noopener">Global MMLU</a>, and <a href="https://arxiv.org/abs/2410.15522" target="blank" rel="external nofollow noopener">M-RewardBench</a>! <img class="emoji" title=":collision:" alt=":collision:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Apr 29, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2504.20879" target="blank" rel="external nofollow noopener">The Leaderboard Illusion</a> is available on Arxiv now. </td> </tr> <tr> <th scope="row">Jan 25, 2025</th> <td> Two papers accepted to ICLR main track: <a href="https://arxiv.org/abs/2408.10914" target="blank" rel="external nofollow noopener">To Code, or Not To Code?</a> and <a href="https://arxiv.org/abs/2411.19799" target="blank" rel="external nofollow noopener">INCLUDE</a>! <img class="emoji" title=":dizzy:" alt=":dizzy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ab.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 20, 2024</th> <td> Senior -&gt; Staff :) </td> </tr> <tr> <th scope="row">Dec 10, 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2412.04261" target="blank" rel="external nofollow noopener">Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier</a> is available on Arxiv now. </td> </tr> <tr> <th scope="row">Sep 25, 2024</th> <td> Our <a href="https://arxiv.org/abs/2311.17295" target="blank" rel="external nofollow noopener">Elo Uncovered</a> paper is accepted to Neurips! <img class="emoji" title=":medal_military:" alt=":medal_military:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f396.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Sep 23, 2024</th> <td> Two papers accepted to EMNLP main track: <a href="https://arxiv.org/abs/2406.18682" target="blank" rel="external nofollow noopener">Multilingual Prism</a> and <a href="https://arxiv.org/abs/2407.01490" target="blank" rel="external nofollow noopener">LLM see, LLM do</a>! <img class="emoji" title=":dizzy:" alt=":dizzy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ab.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Aug 15, 2024</th> <td> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> Our Aya model paper received the ACL <a href="https://x.com/ahmetustun89/status/1823686908087820401" target="blank" rel="external nofollow noopener">Best Paper Award</a>! <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 7, 2024</th> <td> Three papers accepted to ACL main track: <a href="https://arxiv.org/abs/2402.07827" target="blank" rel="external nofollow noopener">Aya Model</a>, <a href="https://arxiv.org/abs/2402.06619" target="blank" rel="external nofollow noopener">Aya Dataset</a>, and <a href="https://arxiv.org/abs/2402.14740" target="blank" rel="external nofollow noopener">RLOO</a>! <img class="emoji" title=":collision:" alt=":collision:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Feb 13, 2024</th> <td> We release <a href="https://cohere.com/research/aya" target="blank" rel="external nofollow noopener">Aya 101</a> <img class="emoji" title=":herb:" alt=":herb:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f33f.png" height="20" width="20"> a massively multilingual dataset and language model. For more details checkout our <a href="https://arxiv.org/abs/2402.06619" target="blank" rel="external nofollow noopener">Data</a> and <a href="https://arxiv.org/abs/2402.07827" target="blank" rel="external nofollow noopener">Model</a> paper. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="singh2025leaderboardillusion" class="col-sm-8"> <div class="title">The Leaderboard Illusion</div> <div class="author"> Shivalika Singh, Yiyang Nan, <a href="https://w4ngatang.github.io/" rel="external nofollow noopener" target="_blank">Alex Wang</a>, Daniel D’Souza, Sayash Kapoor, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, Sanmi Koyejo, Yuntian Deng, <a href="https://www.shaynelongpre.com/" rel="external nofollow noopener" target="_blank">Shayne Longpre</a>, <a href="https://nasmith.github.io/" rel="external nofollow noopener" target="_blank">Noah A. Smith</a>, Beyza Ermis, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.20879" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://cohere.com/research/lmarena" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2025leaderboardillusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Leaderboard Illusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singh, Shivalika and Nan, Yiyang and Wang, Alex and D'Souza, Daniel and Kapoor, Sayash and Üstün, Ahmet and Koyejo, Sanmi and Deng, Yuntian and Longpre, Shayne and Smith, Noah A. and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.20879}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="salazar2025kaleidoscopeinlanguageexamsmassively" class="col-sm-8"> <div class="title">Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</div> <div class="author"> Israfel Salazar, Manuel Fernández Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemiński, Jekaterina Novikova, Luísa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popovič, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel Costa Merlin, Otávio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin fard, Silvia Fernandez, María Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.07072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/kaleidoscope" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">salazar2025kaleidoscopeinlanguageexamsmassively</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Salazar, Israfel and Burda, Manuel Fernández and Islam, Shayekh Bin and Moakhar, Arshia Soltani and Singh, Shivalika and Farestam, Fabian and Romanou, Angelika and Boiko, Danylo and Khullar, Dipika and Zhang, Mike and Krzemiński, Dominik and Novikova, Jekaterina and Shimabucoro, Luísa and Imperial, Joseph Marvin and Maheshwary, Rishabh and Duwal, Sharad and Amayuelas, Alfonso and Rajwal, Swati and Purbey, Jebish and Ruby, Ahmed and Popovič, Nicholas and Suppa, Marek and Wasi, Azmine Toushik and Kadiyala, Ram Mohan Rao and Tsymboi, Olga and Kostritsya, Maksim and Moakhar, Bardia Soltani and da Costa Merlin, Gabriel and Coletti, Otávio Ferracioli and Shiviari, Maral Jabbari and farahani fard, MohammadAmin and Fernandez, Silvia and Grandury, María and Abulkhanov, Dmitry and Sharma, Drishti and Mitri, Andre Guarnier De and Marchezi, Leticia Bossatto and Heydari, Setayesh and Obando-Ceron, Johan and Kohut, Nazar and Ermis, Beyza and Elliott, Desmond and Ferrante, Enzo and Hooker, Sara and Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.07072}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="romanou2024includeevaluatingmultilinguallanguage" class="col-sm-8"> <div class="title">INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</div> <div class="author"> Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, <em>Marzieh Fadaee</em>, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and Antoine Bosselut</div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2411.19799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://huggingface.co/datasets/CohereLabs/include-base-44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="/assets/pdf/romanou_iclr_2025_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">romanou2024includeevaluatingmultilinguallanguage</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Romanou, Angelika and Foroutan, Negar and Sotnikova, Anna and Chen, Zeming and Nelaturu, Sree Harsha and Singh, Shivalika and Maheshwary, Rishabh and Altomare, Micol and Haggag, Mohamed A. and A, Snegha and Amayuelas, Alfonso and Amirudin, Azril Hafizi and Aryabumi, Viraat and Boiko, Danylo and Chang, Michael and Chim, Jenny and Cohen, Gal and Dalmia, Aditya Kumar and Diress, Abraham and Duwal, Sharad and Dzenhaliou, Daniil and Florez, Daniel Fernando Erazo and Farestam, Fabian and Imperial, Joseph Marvin and Islam, Shayekh Bin and Isotalo, Perttu and Jabbarishiviari, Maral and Karlsson, Börje F. and Khalilov, Eldar and Klamm, Christopher and Koto, Fajri and Krzemiński, Dominik and de Melo, Gabriel Adriano and Montariol, Syrielle and Nan, Yiyang and Niklaus, Joel and Novikova, Jekaterina and Ceron, Johan Samir Obando and Paul, Debjit and Ploeger, Esther and Purbey, Jebish and Rajwal, Swati and Ravi, Selvan Sunitha and Rydell, Sara and Santhosh, Roshan and Sharma, Drishti and Skenduli, Marjana Prifti and Moakhar, Arshia Soltani and Moakhar, Bardia Soltani and Tamir, Ran and Tarun, Ayush Kumar and Wasi, Azmine Toushik and Weerasinghe, Thenuka Ovin and Yilmaz, Serhan and Zhang, Mike and Schlag, Imanol and Fadaee, Marzieh and Hooker, Sara and Bosselut, Antoine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2411.19799}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="yu2024diversifyconquerdiversitycentricdata" class="col-sm-8"> <div class="title">Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement</div> <div class="author"> Simon Yu, Liangyu Chen, Sara Ahmadian, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2409.11378" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/Cohere-Labs-Community/iterative-data-selection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement},</span>
  <span class="c">author = {Yu, Simon and Chen, Liangyu and Ahmadian, Sara and Fadaee, Marzieh},</span>
  <span class="c">year = {2024},</span>
  <span class="c">eprint = {2409.11378},</span>
  <span class="c">archiveprefix = {arXiv},</span>
  <span class="c">primaryclass = {cs.CL},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Neurips</abbr></div> <div id="boubdir2023elo" class="col-sm-8"> <div class="title">Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</div> <div class="author"> <a href="https://twitter.com/mellem_boo" rel="external nofollow noopener" target="_blank">Meriem Boubdir</a>, <a href="https://eddotman.github.io/" rel="external nofollow noopener" target="_blank">Edward Kim</a>, Beyza Ermis, <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>, and <em>Marzieh Fadaee</em> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2311.17295" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/elo_neurips24_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">boubdir2023elo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Elo Uncovered: Robustness and Best Practices in Language Model Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Boubdir, Meriem and Kim, Edward and Ermis, Beyza and Hooker, Sara and Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.17295}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="marion2023more" class="col-sm-8"> <div class="title">When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale</div> <div class="author"> <a href="https://maxisawesome.github.io/" rel="external nofollow noopener" target="_blank">Max Marion</a>, <a href="https://ahmetustun.github.io/" rel="external nofollow noopener" target="_blank">Ahmet Üstün</a>, <a href="https://luizapozzobon.github.io/" rel="external nofollow noopener" target="_blank">Luiza Pozzobon</a>, <a href="https://w4ngatang.github.io/" rel="external nofollow noopener" target="_blank">Alex Wang</a>, <em>Marzieh Fadaee</em>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.04564" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">marion2023more</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marion, Max and Üstün, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.04564}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="https://doi.org/10.48550/arxiv.2301.01820" class="col-sm-8"> <div class="title">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</div> <div class="author"> Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, Jakub Zavrel, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2301.01820" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval},</span>
  <span class="c">author = {Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},</span>
  <span class="c">year = {2023},</span>
  <span class="c">publisher = {arXiv},</span>
  <span class="c">doi = {10.48550/ARXIV.2301.01820},</span>
  <span class="c">copyright = {Creative Commons Attribution 4.0 International},</span>
  <span class="c">keywords = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/ws/" rel="external nofollow noopener" target="_blank">NGT</a></abbr></div> <div id="fadaee-monz-2020-unreasonable" class="col-sm-8"> <div class="title">The Unreasonable Volatility of Neural Machine Translation Models</div> <div class="author"> <em>Marzieh Fadaee</em>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the Fourth Workshop on Neural Generation and Translation</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.aclweb.org/anthology/2020.ngt-1.10.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where \textitunexpected changes happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26% and 19% of sentence variations, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-monz-2020-unreasonable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Unreasonable Volatility of Neural Machine Translation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Fourth Workshop on Neural Generation and Translation}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{88--96}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/2020.ngt-1.10.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://aclanthology.org/venues/acl/" rel="external nofollow noopener" target="_blank">ACL</a></abbr></div> <div id="fadaee-bisazza-monz:2017:Short2" class="col-sm-8"> <div class="title">Data Augmentation for Low-Resource Neural Machine Translation</div> <div class="author"> <em>Marzieh Fadaee</em>, <a href="http://www.cs.rug.nl/~bisazza/" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a> </div> <div class="periodical"> <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://aclweb.org/anthology/P17-2090.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">fadaee-bisazza-monz:2017:Short2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Augmentation for Low-Resource Neural Machine Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{567--573}</span><span class="p">,</span>
  <span class="na">bib</span> <span class="p">=</span> <span class="s">{https://www.aclweb.org/anthology/P17-2090.bib}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%72%7A%69%65%68@%63%6F%68%65%72%65.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=NZqs0toAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/marziehf" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/marzieh-fadaee-b7393370" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/mziizm" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a rel="me" href="https://sigmoid.social/@marzieh" title="Mastodon" target="_blank"><i class="fab fa-mastodon"></i></a> </div> <div class="contact-note"> Email is probably the best way to get in touch. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Marzieh Fadaee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?40a752d5efea2d0dae8b9d1dee79d33d"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>